{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mvMWjFCZuiUX"
      },
      "outputs": [],
      "source": [
        "# loading libraries for data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# loading libraries for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from plotnine import *\n",
        "from PIL import Image\n",
        "\n",
        "# import tensorflow and keras packages\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# let's also include different Models, Layers directly from keras\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Dense,Dropout,LSTM,Embedding,Input,GRU\n",
        "\n",
        "# use requests package to download some text\n",
        "import requests\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# url to Romeo and Juliet in text form\n",
        "url = \"https://gutenberg.org/cache/epub/1513/pg1513.txt\"\n",
        "text = requests.get(url).text\n",
        "\n",
        "# clean text\n",
        "text = text[text.find(\"Chapter I.]\")+10:text.find(\"*** END OF THE PROJECT\")] # exclude metadata\n",
        "text = text.lower()\n",
        "print(f\"Length of text: {len(text)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6kbpqWKuvkm",
        "outputId": "e51488a5-6f3d-4f70-c9d4-cb2b3d48f881"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 148586 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# identify unique words in text\n",
        "words = text.split()\n",
        "print(f\"Total words: {len(words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzNlgUIUu6yP",
        "outputId": "f2514cbd-1bce-449f-cec6-b9aacfa09f8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 26093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the two dictionaries\n",
        "vocab = sorted(set(words))\n",
        "print(f\"Unique words: {len(vocab)}\")\n",
        "\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for i, w in enumerate(vocab)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_jn9kYkvIE_",
        "outputId": "911a883e-b6f5-4c72-c53d-58a0a338e965"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words: 5775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_as_int = np.array([word2idx[w] for w in words], dtype=np.int32)\n",
        "print(\"First 20 encoded words:\", text_as_int[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAE1UzNkvMNV",
        "outputId": "7288b77d-6653-4b76-ab63-7d9c5cef58d4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 encoded words: [1507 2120 1504 3446 4060  220 2579 4931 1504 2511 1842 4869 5246 3446\n",
            "  245  248 2468 4869 5196 4609]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 30\n",
        "examples_per_epoch = len(text_as_int) // (seq_length + 1)\n",
        "print(f\"Number of sequences: {examples_per_epoch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK6wexoWvPnk",
        "outputId": "83e79d4f-39f2-4444-c234-a6db93b57371"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = word_dataset.batch(seq_length + 1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "BBCfpEzrP6MY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first words characters in the data\n",
        "for i, item in enumerate(word_dataset.take(10)):\n",
        "    print(item.numpy())\n",
        "\n",
        "# print the first sequence\n",
        "for i, item in enumerate(sequences.take(1)):\n",
        "    print(item.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EbQcyj1ASIhq",
        "outputId": "5c47b7bd-c4bc-40f4-9b1a-09dc64840672"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1507\n",
            "2120\n",
            "1504\n",
            "3446\n",
            "4060\n",
            "220\n",
            "2579\n",
            "4931\n",
            "1504\n",
            "2511\n",
            "[1507 2120 1504 3446 4060  220 2579 4931 1504 2511 1842 4869 5246 3446\n",
            "  245  248 2468 4869 5196 4609  220]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   input_text (first 30 chars)\n",
        "#   target_text (the next 30 chars, shifted by one position)\n",
        "def split_input_target(chunk):\n",
        "    input_seq = chunk[:-1]\n",
        "    target_seq = chunk[1:]\n",
        "    return input_seq, target_seq\n",
        "\n",
        "# apply the function to sequences\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "ud_xYqgISjDV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input shape:\", input_example.shape)\n",
        "    print(\"Target shape:\", target_example.shape)\n",
        "    print(\"First input example (as IDs):\", input_example[0].numpy())\n",
        "    print(\"First target example (as IDs):\", target_example[0].numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nWWb3it4Sl4B",
        "outputId": "a56dcf2c-00d3-4660-e3f1-7db459feffe1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (20,)\n",
            "Target shape: (20,)\n",
            "First input example (as IDs): 1507\n",
            "First target example (as IDs): 2120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "Us27shpNSomF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define hyperparameters for the network\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 512\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(None,)),\n",
        "    Embedding(vocab_size, embedding_dim),\n",
        "    LSTM(rnn_units, return_sequences=True),\n",
        "    LSTM(rnn_units, return_sequences=True),   # new layer\n",
        "    Dropout(0.2),\n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        ")"
      ],
      "metadata": {
        "id": "eT0i6tQiSroz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "history = model.fit(dataset, epochs=20,verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "u_5K1OXoSt64",
        "outputId": "21976d5e-6ba1-4b9e-825e-b4b9dedc23c4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - loss: 8.3786\n",
            "Epoch 2/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 3s/step - loss: 7.1990\n",
            "Epoch 3/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3s/step - loss: 7.0193\n",
            "Epoch 4/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - loss: 6.9842\n",
            "Epoch 5/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - loss: 6.9305\n",
            "Epoch 6/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - loss: 6.8900\n",
            "Epoch 7/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - loss: 6.7877\n",
            "Epoch 8/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - loss: 6.7198\n",
            "Epoch 9/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - loss: 6.6780\n",
            "Epoch 10/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - loss: 6.6123\n",
            "Epoch 11/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - loss: 6.5464\n",
            "Epoch 12/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - loss: 6.4903\n",
            "Epoch 13/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - loss: 6.4464\n",
            "Epoch 14/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - loss: 6.3983\n",
            "Epoch 15/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - loss: 6.3263\n",
            "Epoch 16/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - loss: 6.2492\n",
            "Epoch 17/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - loss: 6.2076\n",
            "Epoch 18/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - loss: 6.1171\n",
            "Epoch 19/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - loss: 6.0795\n",
            "Epoch 20/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - loss: 6.0125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "5I0q3-czTHos",
        "outputId": "7bd0055f-5fab-4721-b2f0-b62b67ca7a4d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m1,478,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m2,099,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5775\u001b[0m)     │     \u001b[38;5;34m2,962,575\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,478,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5775</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,962,575</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,345,263\u001b[0m (92.87 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,345,263</span> (92.87 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,115,087\u001b[0m (30.96 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,115,087</span> (30.96 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m16,230,176\u001b[0m (61.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,230,176</span> (61.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_seq, num_generate=50, temperature=1.0):\n",
        "    # Tokenize the starting sequence into words\n",
        "    input_eval = [word2idx.get(w, 0) for w in start_seq.lower().split()]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    generated_words = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model.predict(input_eval, verbose=0)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions[-1:], num_samples=1)[0, 0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        generated_words.append(idx2word[predicted_id])\n",
        "\n",
        "    return start_seq + ' ' + ' '.join(generated_words)"
      ],
      "metadata": {
        "id": "QNLplJbWTQBc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, \"romeo\", 40, temperature=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "HKxwnkflTT4T",
        "outputId": "0fb9011f-6f48-4e4c-c4b3-c47920ad7b0b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'romeo meanest this foot bride! earthen pardon compare and men. romeo. that eye; and scene case mov’d? vitae. county or gone! than in make how and company: o’clock than sir. stick beats satisfied. sin, eyes, her child than will bed read'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = generate_text(model, \"juliet\", num_generate=1000, temperature=0.5)\n",
        "output = output.split(\".\")\n",
        "for sentence in output:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "G4yC0P4FTyuU",
        "outputId": "572dd889-c331-430e-dc5c-225e7a322cc5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "juliet to a lawrence\n",
            " and thou art, lets dovehouse beauties: or the father highway doves peace, nurse\n",
            " and montague\n",
            " and this mother romeo\n",
            " the sentence course; full for and and the will the lawrence\n",
            " the bed\n",
            " report\n",
            " grudge the hand\n",
            " the lawrence\n",
            " the hurt and my minute in my lawrence\n",
            " the man and a door, clouds, gentler virtue and in and i i will the tomb; leg taker the man fares that in the hall, youthful good lawrence\n",
            " and for to a errand\n",
            " augmenting and a will and the man with and be an old lady and and am nurse\n",
            " have thou lawrence\n",
            " this will the man and and and the lawrence\n",
            " with i are and the much\n",
            " the my lawrence\n",
            " heaven for with love, and romeo, her a hath and my night, the man of the lawrence\n",
            " the letter’s that the lawrence\n",
            " and the thing gracious judgment and with to these love juliet, of and and in have this traces, robes brain\n",
            " that in and to my mother?’ ancestors i a man i have a fair flesh\n",
            " the sentence chinks\n",
            " fish and thou to a foul nuptial dried and the time her fearful black the fearful a lawrence\n",
            " if and a man this enmity\n",
            " carrion another’s have to romeo\n",
            " i spoke hill pleading ta’en, and to and to to such a man the man and and i the do a love for and romeo\n",
            " in will and a very love to and and in a lawrence\n",
            " in shall and in the thursday, have; the lawrence\n",
            " of romeo, i make i man for would this thou my soul and [_aside\n",
            "_] with the man to a lawrence\n",
            " wilt and be a would is and now, and the a lawrence\n",
            " a a garden\n",
            " my that prince\n",
            " romeo! abus’d bread, to is by and that my backward patient am and not her lawrence\n",
            " and do is and a lawrence\n",
            " and is my the night’s good is the art in night and is to of in the lawrence\n",
            " and i romeo, is have and to the lawrence\n",
            " have the should her like with the am: this have and the lawrence\n",
            " the your different i the world my lawrence\n",
            " to and the gone! the lawrence\n",
            " for note i time the soul in to will is and in my lawrence\n",
            " i to eye; and and that and of lady to how and is the a man to and the soul this lawrence\n",
            " a veins a a soul and that in and the lawrence\n",
            " a courteous, deeds hairs, lawrence\n",
            " thy romeo\n",
            " to and her have thou thou in thy lawrence\n",
            " that my lawrence\n",
            " the lawrence\n",
            " did and of that and lawrence\n",
            " the soul lady men’s that that the man with madam, and of and and that to this tributary that of the lawrence\n",
            " this man and a lawrence\n",
            " how the lawrence\n",
            " to and in and the while? the will the man and and and my lawrence\n",
            " the thy lawrence\n",
            " the night\n",
            " in but and my lawrence\n",
            " and and and and the day and thou of and and a man blood? can, sun! in to and and and and the bridal and and my a lawrence\n",
            " is and and what thou and the wilt that joy thou in to a joyful of to in and a lawrence\n",
            " and and the that thy shall and prince, gregory\n",
            " of discords\n",
            " and and in the man and is and my lawrence\n",
            " but do the lawrence\n",
            " these spoke graves, thyself\n",
            " winter cheer, minion before street, is and and of the good lawrence\n",
            " i are i a lawrence\n",
            " and is and and in in and the house\n",
            " using wives soar late, the man and but that to and the courteous, glove by and and these lawrence\n",
            " my thursday, her open the lawrence\n",
            " my certain thine have and our burying a lawrence\n",
            " thou the tybalt to to and and the make for to this make and and thou that in will that and and lady ere my lawrence\n",
            " to the am purpose, is my juliet’s weeping, is pale sheath\n",
            " the lawrence\n",
            " the knew and to lady time that a whining the have and and love of the lawrence\n",
            " and i with and we in the in the word, and love and and and to and a man and and of this thou this sentence treasure deeds heareth paris, to and but and and a world and romeo, i lawrence\n",
            " thou safety and with that and and have the lawrence\n",
            " lawrence\n",
            " and with that the numbers tree and and for and and in day and of and the cell\n",
            " of i romeo, my lawrence\n",
            " man a lawrence\n",
            " good minute i lawrence\n",
            " and the soul and in a view crow, and the lawrence\n",
            " and the have a sentence confines street\n",
            " ’twould what i is the eye rage, the flower jocund vile lawrence\n",
            " the man with the last\n",
            " a man a joyful stroke howling have the discreet, and to to is to a thursday, romeo\n",
            " for and and save of thou will the marriage, this and and and and will a lady secret? the last\n",
            " longer in and a lawrence\n",
            " to to and and and shall and i my lawrence\n",
            " and and the an o lawrence\n",
            " that by i and is of of juliet\n",
            " and in that to on is lady to of were the lawrence\n",
            " have and the lawrence\n",
            " dead that of and what my ebook power what thou and nurse\n",
            " and and i this is the afternoon, beams; me purpose, night\n",
            " and and i the i\n",
            " and and are that thou i this lawrence\n",
            " the lawrence\n",
            " no sun\n",
            " of and a soul and and love is to and and what and is in and he and and and the too reason winning they night and and romeo, and the envious am all the that a lawrence\n",
            " the were a lawrence\n",
            " a lawrence\n",
            " the lawrence\n",
            " to and i and have in is and that to the i\n",
            " and that to not was and and i to gentle\n"
          ]
        }
      ]
    }
  ]
}